Epoch [1/2], Loss: 1.3781
Epoch [1/2], Loss: 18.0107
Epoch [1/2], Loss: 12.5390
Epoch [1/2], Loss: 7.0882
Epoch [1/2], Loss: 3.7829
Epoch [1/2], Loss: 2.0707
Epoch [1/2], Loss: 2.2042
Epoch [1/2], Loss: 1.7270
Epoch [1/2], Loss: 1.5513
Epoch [1/2], Loss: 1.2330
Epoch [1/2], Loss: 1.3204
Epoch [1/2], Loss: 1.3496
Epoch [1/2], Loss: 1.2986
Epoch [1/2], Loss: 1.1015
Epoch [1/2], Loss: 1.2948
Epoch [1/2], Loss: 0.9894
Epoch [1/2], Loss: 1.2126
Epoch [1/2], Loss: 1.0827
Epoch [1/2], Loss: 1.0898
Epoch [1/2], Loss: 1.2118
Epoch [1/2], Loss: 1.0369
Epoch [1/2], Loss: 1.0137
Epoch [1/2], Loss: 1.0686
Epoch [1/2], Loss: 1.1088
Epoch [1/2], Loss: 1.0823
Epoch [1/2], Loss: 1.3160
Epoch [1/2], Loss: 1.1718
Epoch [1/2], Loss: 1.0907
Epoch [1/2], Loss: 1.1533
Epoch [1/2], Loss: 0.8790
Epoch [1/2], Loss: 1.1235
Epoch [1/2], Loss: 0.8819
Epoch [1/2], Loss: 1.1455
Epoch [1/2], Loss: 0.9089
Epoch [1/2], Loss: 0.8923
Epoch [1/2], Loss: 0.6433
Epoch [1/2], Loss: 1.0957
Epoch [1/2], Loss: 1.0232
Epoch [1/2], Loss: 0.8243
Epoch [1/2], Loss: 1.1037
Epoch [1/2], Loss: 0.8925
Epoch [1/2], Loss: 1.0156
Epoch [1/2], Loss: 0.8621
Epoch [1/2], Loss: 0.8323
Epoch [1/2], Loss: 1.0203
Epoch [1/2], Loss: 0.7964
Epoch [1/2], Loss: 0.9277
Epoch [1/2], Loss: 1.0411
Epoch [1/2], Loss: 0.9279
Epoch [1/2], Loss: 0.8318
Epoch [1/2], Loss: 0.7555
Epoch [1/2], Loss: 0.9963
Epoch [1/2], Loss: 0.9735
Epoch [1/2], Loss: 0.9593
Epoch [1/2], Loss: 0.9637
Epoch [1/2], Loss: 0.7965
Epoch [1/2], Loss: 1.0783
Epoch [1/2], Loss: 0.7956
Epoch [1/2], Loss: 0.7363
Epoch [1/2], Loss: 0.6978
Epoch [1/2], Loss: 0.8918
Epoch [1/2], Loss: 0.6464
Epoch [1/2], Loss: 0.7747
Epoch [1/2], Loss: 0.9181
Epoch [1/2], Loss: 0.7367
Epoch [1/2], Loss: 0.6972
Epoch [1/2], Loss: 0.5473
Epoch [1/2], Loss: 0.7893
Epoch [1/2], Loss: 0.8832
Epoch [1/2], Loss: 0.8209
Epoch [1/2], Loss: 0.5263
Epoch [1/2], Loss: 1.1924
Epoch [1/2], Loss: 0.8155
Epoch [1/2], Loss: 0.7953
Epoch [1/2], Loss: 0.6527
Epoch [1/2], Loss: 0.5318
Epoch [1/2], Loss: 0.5057
Epoch [1/2], Loss: 0.8079
Epoch [1/2], Loss: 0.8023
Epoch [1/2], Loss: 0.7476
Epoch [1/2], Loss: 0.6761
Epoch [1/2], Loss: 1.1444
Epoch [1/2], Loss: 0.6961
Epoch [1/2], Loss: 0.6624
Epoch [1/2], Loss: 0.9686
Epoch [1/2], Loss: 1.1566
Epoch [1/2], Loss: 0.4819
Epoch [1/2], Loss: 0.5331
Epoch [1/2], Loss: 0.6109
Epoch [1/2], Loss: 0.5874
Epoch [1/2], Loss: 0.7301
Epoch [1/2], Loss: 1.0706
Epoch [1/2], Loss: 0.8105
Epoch [1/2], Loss: 0.8396
Epoch [1/2], Loss: 0.3760
Epoch [1/2], Loss: 0.6755
Epoch [1/2], Loss: 0.6217
Epoch [1/2], Loss: 0.8480
Epoch [1/2], Loss: 0.5814
Epoch [1/2], Loss: 0.7535
Epoch [1/2], Loss: 0.7265
Epoch [1/2], Loss: 0.8550
Epoch [1/2], Loss: 0.7289
Epoch [1/2], Loss: 0.6884
Epoch [1/2], Loss: 0.7899
Epoch [1/2], Loss: 0.7592
Epoch [1/2], Loss: 0.3781
Epoch [1/2], Loss: 0.5335
Epoch [1/2], Loss: 0.7915
Epoch [1/2], Loss: 0.9458
Epoch [1/2], Loss: 0.6133
Epoch [1/2], Loss: 0.8797
Epoch [1/2], Loss: 0.6414
Epoch [1/2], Loss: 0.5619
Epoch [1/2], Loss: 0.8718
Epoch [1/2], Loss: 0.5094
Epoch [1/2], Loss: 0.6563
Epoch [1/2], Loss: 0.5538
Epoch [1/2], Loss: 0.5227
Epoch [1/2], Loss: 1.3069
Epoch [1/2], Loss: 0.7374
Epoch [1/2], Loss: 0.6477
Epoch [1/2], Loss: 0.6317
Epoch [1/2], Loss: 0.7524
Epoch [1/2], Loss: 0.5372
Epoch [1/2], Loss: 0.5559
Epoch [1/2], Loss: 0.6142
Epoch [1/2], Loss: 0.7114
Epoch [1/2], Loss: 0.8075
Epoch [1/2], Loss: 0.4875
Epoch [1/2], Loss: 0.4939
Epoch [1/2], Loss: 0.5621
Epoch [1/2], Loss: 0.5073
Epoch [1/2], Loss: 0.8106
Epoch [1/2], Loss: 1.0878
Epoch [1/2], Loss: 0.3989
Epoch [1/2], Loss: 0.3517
Epoch [1/2], Loss: 0.4135
Epoch [1/2], Loss: 0.6201
Epoch [1/2], Loss: 0.5238
Epoch [1/2], Loss: 0.7234
Epoch [1/2], Loss: 0.6560
Epoch [1/2], Loss: 0.4606
Epoch [1/2], Loss: 0.4188
Epoch [1/2], Loss: 0.3353
Epoch [1/2], Loss: 0.9146
Epoch [1/2], Loss: 0.6799
Epoch [1/2], Loss: 0.7352
Epoch [1/2], Loss: 0.6956
Epoch [1/2], Loss: 0.4530
Epoch [1/2], Loss: 0.8338
Epoch [1/2], Loss: 0.6673
Epoch [1/2], Loss: 0.7466
Epoch [1/2], Loss: 0.6258
Epoch [1/2], Loss: 0.6882
Epoch [1/2], Loss: 0.7543
Epoch [1/2], Loss: 0.7124
Epoch [1/2], Loss: 0.5483
Epoch [1/2], Loss: 0.6355
Epoch [1/2], Loss: 0.8043
Epoch [1/2], Loss: 0.9115
Epoch [1/2], Loss: 0.4593
Epoch [1/2], Loss: 0.8533
Epoch [1/2], Loss: 0.8045
Epoch [1/2], Loss: 0.5285
Epoch [1/2], Loss: 0.4865
Epoch [1/2], Loss: 0.5548
Epoch [1/2], Loss: 0.5591
Epoch [1/2], Loss: 0.6007
Epoch [1/2], Loss: 0.5935
Epoch [1/2], Loss: 0.6858
Epoch [1/2], Loss: 0.6736
Epoch [1/2], Loss: 0.5499
Epoch [1/2], Loss: 0.8625
Epoch [1/2], Loss: 0.6867
Epoch [1/2], Loss: 0.6475
Epoch [1/2], Loss: 0.5285
Epoch [1/2], Loss: 0.3876
Traceback (most recent call last):
  File "/home/reva/CMPM17-ML/brain-cancer-model/training_loop.py", line 317, in <module>
    val_pred = model(val_loader)
               ^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/CMPM17-ML/brain-cancer-model/training_loop.py", line 236, in forward
    x = self.pool(self.relu(self.conv1(x)))
                            ^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
TypeError: conv2d() received an invalid combination of arguments - got (DataLoader, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mDataLoader[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = "valid", tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mDataLoader[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)
