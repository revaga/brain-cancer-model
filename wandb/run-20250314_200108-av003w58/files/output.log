Epoch [1/2], Loss: 1.3889
Epoch [1/2], Loss: 7.0200
Epoch [1/2], Loss: 5.1230
Epoch [1/2], Loss: 2.1549
Epoch [1/2], Loss: 1.9261
Epoch [1/2], Loss: 1.9548
Epoch [1/2], Loss: 1.8235
Epoch [1/2], Loss: 1.4680
Epoch [1/2], Loss: 1.3735
Epoch [1/2], Loss: 1.4516
Epoch [1/2], Loss: 1.4204
Epoch [1/2], Loss: 1.3534
Epoch [1/2], Loss: 1.3993
Epoch [1/2], Loss: 1.3766
Epoch [1/2], Loss: 1.3524
Epoch [1/2], Loss: 1.3387
Epoch [1/2], Loss: 1.3429
Epoch [1/2], Loss: 1.1927
Epoch [1/2], Loss: 1.4456
Epoch [1/2], Loss: 1.2339
Epoch [1/2], Loss: 1.3794
Epoch [1/2], Loss: 1.2616
Epoch [1/2], Loss: 1.1829
Epoch [1/2], Loss: 1.2503
Epoch [1/2], Loss: 1.2189
Epoch [1/2], Loss: 1.3141
Epoch [1/2], Loss: 1.0099
Epoch [1/2], Loss: 1.1011
Epoch [1/2], Loss: 1.0932
Epoch [1/2], Loss: 1.0986
Epoch [1/2], Loss: 1.0279
Epoch [1/2], Loss: 0.9776
Epoch [1/2], Loss: 0.8328
Epoch [1/2], Loss: 0.8512
Epoch [1/2], Loss: 1.1227
Epoch [1/2], Loss: 1.1886
Epoch [1/2], Loss: 1.2416
Epoch [1/2], Loss: 0.9395
Epoch [1/2], Loss: 0.9246
Epoch [1/2], Loss: 0.9181
Epoch [1/2], Loss: 0.8995
Epoch [1/2], Loss: 1.1038
Epoch [1/2], Loss: 0.9113
Epoch [1/2], Loss: 0.8916
Epoch [1/2], Loss: 1.0503
Epoch [1/2], Loss: 1.1722
Epoch [1/2], Loss: 1.5441
Epoch [1/2], Loss: 0.8633
Epoch [1/2], Loss: 0.9222
Epoch [1/2], Loss: 1.0315
Epoch [1/2], Loss: 1.1150
Epoch [1/2], Loss: 1.0964
Epoch [1/2], Loss: 0.7722
Epoch [1/2], Loss: 1.1049
Epoch [1/2], Loss: 0.8306
Epoch [1/2], Loss: 1.0021
Epoch [1/2], Loss: 1.0238
Epoch [1/2], Loss: 0.8021
Epoch [1/2], Loss: 1.1253
Epoch [1/2], Loss: 0.7805
Epoch [1/2], Loss: 0.5760
Epoch [1/2], Loss: 0.6763
Epoch [1/2], Loss: 0.9031
Epoch [1/2], Loss: 1.3632
Epoch [1/2], Loss: 1.0777
Epoch [1/2], Loss: 0.6948
Epoch [1/2], Loss: 1.0433
Epoch [1/2], Loss: 0.4347
Epoch [1/2], Loss: 0.7658
Epoch [1/2], Loss: 0.8001
Epoch [1/2], Loss: 0.7574
Epoch [1/2], Loss: 0.9879
Epoch [1/2], Loss: 0.8743
Epoch [1/2], Loss: 0.6954
Epoch [1/2], Loss: 0.8395
Epoch [1/2], Loss: 0.9421
Epoch [1/2], Loss: 0.7416
Epoch [1/2], Loss: 1.0569
Epoch [1/2], Loss: 0.8652
Epoch [1/2], Loss: 1.0630
Epoch [1/2], Loss: 0.8554
Epoch [1/2], Loss: 0.7351
Epoch [1/2], Loss: 0.7235
Epoch [1/2], Loss: 0.9015
Epoch [1/2], Loss: 0.9104
Epoch [1/2], Loss: 0.9151
Epoch [1/2], Loss: 0.8240
Epoch [1/2], Loss: 0.9241
Epoch [1/2], Loss: 0.9085
Epoch [1/2], Loss: 0.6877
Epoch [1/2], Loss: 1.3658
Epoch [1/2], Loss: 0.9854
Epoch [1/2], Loss: 0.7730
Epoch [1/2], Loss: 0.7157
Epoch [1/2], Loss: 0.7560
Epoch [1/2], Loss: 0.6620
Epoch [1/2], Loss: 0.7288
Epoch [1/2], Loss: 0.8139
Epoch [1/2], Loss: 0.8520
Epoch [1/2], Loss: 0.8367
Epoch [1/2], Loss: 0.9082
Epoch [1/2], Loss: 0.8236
Epoch [1/2], Loss: 0.7707
Epoch [1/2], Loss: 0.8189
Epoch [1/2], Loss: 0.8508
Epoch [1/2], Loss: 0.9602
Epoch [1/2], Loss: 0.7067
Epoch [1/2], Loss: 0.6443
Epoch [1/2], Loss: 0.7874
Epoch [1/2], Loss: 0.8216
Epoch [1/2], Loss: 0.8713
Epoch [1/2], Loss: 0.7925
Epoch [1/2], Loss: 0.7695
Epoch [1/2], Loss: 1.0011
Epoch [1/2], Loss: 0.7034
Epoch [1/2], Loss: 0.5778
Epoch [1/2], Loss: 0.6811
Epoch [1/2], Loss: 0.5485
Epoch [1/2], Loss: 0.6992
Epoch [1/2], Loss: 0.6698
Epoch [1/2], Loss: 0.7205
Epoch [1/2], Loss: 0.5721
Epoch [1/2], Loss: 0.7693
Epoch [1/2], Loss: 1.2034
Epoch [1/2], Loss: 0.7376
Epoch [1/2], Loss: 0.7259
Epoch [1/2], Loss: 0.8662
Epoch [1/2], Loss: 0.7661
Epoch [1/2], Loss: 0.5739
Epoch [1/2], Loss: 0.7709
Epoch [1/2], Loss: 0.6545
Epoch [1/2], Loss: 0.5534
Epoch [1/2], Loss: 0.5979
Epoch [1/2], Loss: 0.6913
Epoch [1/2], Loss: 0.5256
Epoch [1/2], Loss: 0.8539
Epoch [1/2], Loss: 0.3845
Epoch [1/2], Loss: 0.5136
Epoch [1/2], Loss: 0.7635
Epoch [1/2], Loss: 0.5100
Epoch [1/2], Loss: 0.4317
Epoch [1/2], Loss: 0.7509
Epoch [1/2], Loss: 0.7870
Epoch [1/2], Loss: 0.7051
Epoch [1/2], Loss: 0.9940
Epoch [1/2], Loss: 0.6602
Epoch [1/2], Loss: 0.5733
Epoch [1/2], Loss: 0.7996
Epoch [1/2], Loss: 0.4475
Epoch [1/2], Loss: 0.8073
Epoch [1/2], Loss: 0.7331
Epoch [1/2], Loss: 0.5748
Epoch [1/2], Loss: 0.5710
Epoch [1/2], Loss: 0.4376
Epoch [1/2], Loss: 0.5837
Epoch [1/2], Loss: 0.6986
Epoch [1/2], Loss: 0.8826
Epoch [1/2], Loss: 0.7745
Epoch [1/2], Loss: 0.6706
Epoch [1/2], Loss: 0.9227
Epoch [1/2], Loss: 0.7877
Epoch [1/2], Loss: 0.8189
Epoch [1/2], Loss: 0.7527
Epoch [1/2], Loss: 0.5773
Epoch [1/2], Loss: 0.6067
Epoch [1/2], Loss: 0.4975
Epoch [1/2], Loss: 0.9484
Epoch [1/2], Loss: 0.6144
Epoch [1/2], Loss: 0.6758
Epoch [1/2], Loss: 0.5658
Epoch [1/2], Loss: 0.5869
Epoch [1/2], Loss: 0.4918
Epoch [1/2], Loss: 0.5687
Epoch [1/2], Loss: 0.5504
Epoch [1/2], Loss: 0.5942
Epoch [1/2], Loss: 0.5423
Epoch [1/2], Loss: 0.6428
Epoch [1/2], Loss: 0.0399
Traceback (most recent call last):
  File "/home/reva/CMPM17-ML/brain-cancer-model/training_loop.py", line 315, in <module>
    val_pred = model(val_loader)
               ^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/CMPM17-ML/brain-cancer-model/training_loop.py", line 234, in forward
    x = self.pool(self.relu(self.conv1(x)))
                            ^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
TypeError: conv2d() received an invalid combination of arguments - got (DataLoader, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mDataLoader[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = "valid", tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mDataLoader[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)
