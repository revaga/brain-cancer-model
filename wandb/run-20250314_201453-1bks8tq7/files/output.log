Epoch [1/2], Loss: 1.3945
Epoch [1/2], Loss: 7.3972
Epoch [1/2], Loss: 7.3346
Epoch [1/2], Loss: 4.8633
Epoch [1/2], Loss: 2.4695
Epoch [1/2], Loss: 1.3712
Epoch [1/2], Loss: 1.3227
Epoch [1/2], Loss: 1.3117
Epoch [1/2], Loss: 1.2331
Epoch [1/2], Loss: 1.2180
Epoch [1/2], Loss: 1.0883
Epoch [1/2], Loss: 1.3298
Epoch [1/2], Loss: 1.0560
Epoch [1/2], Loss: 1.2441
Epoch [1/2], Loss: 1.2624
Epoch [1/2], Loss: 1.1690
Epoch [1/2], Loss: 1.1686
Epoch [1/2], Loss: 1.0122
Epoch [1/2], Loss: 1.0868
Epoch [1/2], Loss: 0.9644
Epoch [1/2], Loss: 1.2628
Epoch [1/2], Loss: 0.9910
Epoch [1/2], Loss: 1.0265
Epoch [1/2], Loss: 1.0137
Epoch [1/2], Loss: 1.1053
Epoch [1/2], Loss: 1.1325
Epoch [1/2], Loss: 1.1038
Epoch [1/2], Loss: 0.8794
Epoch [1/2], Loss: 1.1103
Epoch [1/2], Loss: 0.9625
Epoch [1/2], Loss: 0.9203
Epoch [1/2], Loss: 1.0249
Epoch [1/2], Loss: 1.1865
Epoch [1/2], Loss: 0.8578
Epoch [1/2], Loss: 0.9166
Epoch [1/2], Loss: 1.0624
Epoch [1/2], Loss: 1.0989
Epoch [1/2], Loss: 0.8112
Epoch [1/2], Loss: 0.6247
Epoch [1/2], Loss: 0.8060
Epoch [1/2], Loss: 0.9369
Epoch [1/2], Loss: 0.8526
Epoch [1/2], Loss: 0.8728
Epoch [1/2], Loss: 0.7124
Epoch [1/2], Loss: 0.5984
Epoch [1/2], Loss: 1.0112
Epoch [1/2], Loss: 1.0436
Epoch [1/2], Loss: 0.8867
Epoch [1/2], Loss: 0.8953
Epoch [1/2], Loss: 0.8032
Epoch [1/2], Loss: 0.8626
Epoch [1/2], Loss: 0.7121
Epoch [1/2], Loss: 0.8029
Epoch [1/2], Loss: 0.6195
Epoch [1/2], Loss: 0.8025
Epoch [1/2], Loss: 1.2121
Epoch [1/2], Loss: 0.8336
Epoch [1/2], Loss: 0.8343
Epoch [1/2], Loss: 0.9516
Epoch [1/2], Loss: 0.9711
Epoch [1/2], Loss: 1.0474
Epoch [1/2], Loss: 1.2286
Epoch [1/2], Loss: 0.9132
Epoch [1/2], Loss: 0.9319
Epoch [1/2], Loss: 0.9458
Epoch [1/2], Loss: 1.0068
Epoch [1/2], Loss: 0.9362
Epoch [1/2], Loss: 1.1514
Epoch [1/2], Loss: 0.9491
Epoch [1/2], Loss: 0.6073
Epoch [1/2], Loss: 0.8535
Epoch [1/2], Loss: 1.1245
Epoch [1/2], Loss: 0.8901
Epoch [1/2], Loss: 0.6843
Epoch [1/2], Loss: 0.9416
Epoch [1/2], Loss: 0.9491
Epoch [1/2], Loss: 0.8373
Epoch [1/2], Loss: 0.7437
Epoch [1/2], Loss: 0.7908
Epoch [1/2], Loss: 0.9876
Epoch [1/2], Loss: 0.6668
Epoch [1/2], Loss: 0.8611
Epoch [1/2], Loss: 0.8804
Epoch [1/2], Loss: 0.7283
Epoch [1/2], Loss: 0.8099
Epoch [1/2], Loss: 0.5947
Epoch [1/2], Loss: 0.6552
Epoch [1/2], Loss: 0.4126
Epoch [1/2], Loss: 0.7625
Epoch [1/2], Loss: 1.3875
Epoch [1/2], Loss: 0.6945
Epoch [1/2], Loss: 0.6419
Epoch [1/2], Loss: 0.7675
Epoch [1/2], Loss: 0.9747
Epoch [1/2], Loss: 1.0836
Epoch [1/2], Loss: 0.7426
Epoch [1/2], Loss: 0.6906
Epoch [1/2], Loss: 0.8424
Epoch [1/2], Loss: 0.9464
Epoch [1/2], Loss: 0.9725
Epoch [1/2], Loss: 0.8572
Epoch [1/2], Loss: 1.0175
Epoch [1/2], Loss: 0.8646
Epoch [1/2], Loss: 0.7682
Epoch [1/2], Loss: 0.8429
Epoch [1/2], Loss: 0.6954
Epoch [1/2], Loss: 1.1425
Epoch [1/2], Loss: 0.7589
Epoch [1/2], Loss: 0.9730
Epoch [1/2], Loss: 0.8175
Epoch [1/2], Loss: 0.7606
Epoch [1/2], Loss: 0.8548
Epoch [1/2], Loss: 0.8424
Epoch [1/2], Loss: 0.8338
Epoch [1/2], Loss: 0.5999
Epoch [1/2], Loss: 0.9766
Epoch [1/2], Loss: 0.7972
Epoch [1/2], Loss: 1.0231
Epoch [1/2], Loss: 0.7989
Epoch [1/2], Loss: 0.6326
Epoch [1/2], Loss: 0.6404
Epoch [1/2], Loss: 0.7952
Epoch [1/2], Loss: 0.6061
Epoch [1/2], Loss: 0.7929
Epoch [1/2], Loss: 0.5533
Epoch [1/2], Loss: 0.8936
Epoch [1/2], Loss: 0.9127
Epoch [1/2], Loss: 0.6585
Epoch [1/2], Loss: 0.9392
Epoch [1/2], Loss: 0.5689
Epoch [1/2], Loss: 0.7349
Epoch [1/2], Loss: 0.7252
Epoch [1/2], Loss: 0.8451
Epoch [1/2], Loss: 0.7586
Epoch [1/2], Loss: 0.4651
Epoch [1/2], Loss: 0.6444
Epoch [1/2], Loss: 0.7417
Epoch [1/2], Loss: 0.7101
Epoch [1/2], Loss: 0.7229
Epoch [1/2], Loss: 0.7890
Epoch [1/2], Loss: 0.7722
Epoch [1/2], Loss: 0.8088
Epoch [1/2], Loss: 0.8981
Epoch [1/2], Loss: 0.9829
Epoch [1/2], Loss: 0.8383
Epoch [1/2], Loss: 0.6903
Epoch [1/2], Loss: 0.4493
Epoch [1/2], Loss: 0.7249
Epoch [1/2], Loss: 0.7318
Epoch [1/2], Loss: 0.5800
Epoch [1/2], Loss: 0.6595
Epoch [1/2], Loss: 0.6247
Epoch [1/2], Loss: 0.5688
Epoch [1/2], Loss: 0.5022
Epoch [1/2], Loss: 0.9943
Epoch [1/2], Loss: 0.6153
Epoch [1/2], Loss: 0.5728
Epoch [1/2], Loss: 0.5401
Epoch [1/2], Loss: 0.6789
Epoch [1/2], Loss: 0.7179
Epoch [1/2], Loss: 0.5746
Epoch [1/2], Loss: 0.7021
Epoch [1/2], Loss: 0.6471
Epoch [1/2], Loss: 1.0814
Epoch [1/2], Loss: 0.6690
Epoch [1/2], Loss: 0.8702
Epoch [1/2], Loss: 0.9347
Epoch [1/2], Loss: 0.5919
Epoch [1/2], Loss: 0.6619
Epoch [1/2], Loss: 0.6924
Epoch [1/2], Loss: 0.5959
Epoch [1/2], Loss: 0.6938
Epoch [1/2], Loss: 0.8785
Epoch [1/2], Loss: 0.6387
Epoch [1/2], Loss: 0.6654
Epoch [1/2], Loss: 0.7439
Epoch [1/2], Loss: 0.7271
Epoch [1/2], Loss: 0.8256
Traceback (most recent call last):
  File "/home/reva/CMPM17-ML/brain-cancer-model/training_loop.py", line 319, in <module>
    val_pred = model(val_loader)
               ^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/CMPM17-ML/brain-cancer-model/training_loop.py", line 236, in forward
    x = self.pool(self.relu(self.conv1(x)))
                            ^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/reva/VENV-NAME/lib/python3.12/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
           ^^^^^^^^^
TypeError: conv2d() received an invalid combination of arguments - got (DataLoader, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mDataLoader[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)
 * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = "valid", tuple of ints dilation = 1, int groups = 1)
      didn't match because some of the arguments have invalid types: ([31;1mDataLoader[0m, [31;1mParameter[0m, [31;1mParameter[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mtuple of (int, int)[0m, [31;1mint[0m)
